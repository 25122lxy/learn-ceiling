(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{332:function(a,t,e){"use strict";e.r(t);var s=e(1),r=Object(s.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"数据采集工具"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据采集工具"}},[a._v("#")]),a._v(" 数据采集工具")]),a._v(" "),t("h2",{attrs:{id:"flume"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flume"}},[a._v("#")]),a._v(" Flume")]),a._v(" "),t("h3",{attrs:{id:"_1-基本组成"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-基本组成"}},[a._v("#")]),a._v(" 1.基本组成")]),a._v(" "),t("ol",[t("li",[t("p",[t("code",[a._v("taildir source")])]),a._v(" "),t("ol",[t("li",[a._v("断点续传、多目录")]),a._v(" "),t("li",[a._v("那个版本：Apache 1.7")]),a._v(" "),t("li",[a._v("原理：持久化了"),t("code",[a._v("offset")]),a._v("到磁盘文件")])])]),a._v(" "),t("li",[t("p",[t("code",[a._v("channel")]),a._v("（file channel /memory channel/kafka channel）")]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("channel")]),a._v(" "),t("th"),a._v(" "),t("th")])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("memory")]),a._v(" "),t("td",[a._v("基于内存")]),a._v(" "),t("td",[a._v("效率高、可靠性低，默认100个event")])]),a._v(" "),t("tr",[t("td",[a._v("file")]),a._v(" "),t("td",[a._v("基于磁盘")]),a._v(" "),t("td",[a._v("效率低、可靠性高，默认100万个event")])]),a._v(" "),t("tr",[t("td",[a._v("kafka")]),a._v(" "),t("td",[a._v("基于kafka磁盘")]),a._v(" "),t("td",[a._v("效率高、可靠性高")])])])]),a._v(" "),t("ol",[t("li",[t("code",[a._v("kafka channel")]),a._v(">"),t("code",[a._v("memory channel")]),a._v("+"),t("code",[a._v("kafka sink")])]),a._v(" "),t("li",[a._v("有一个bug\n"),t("ol",[t("li",[t("code",[a._v("kafkachannel")]),a._v("1.6产生的，有bug，携带header信息，1.7版本的时候解决了bug")])])]),a._v(" "),t("li",[a._v("怎么选？\n"),t("ol",[t("li",[a._v("如果对接"),t("code",[a._v("kafka")]),a._v("优先"),t("code",[a._v("kafka channel")])]),a._v(" "),t("li",[a._v("如果对接不是kafka\n"),t("ol",[t("li",[a._v("一般企业、普通日志，选效率高的："),t("code",[a._v("memory")])]),a._v(" "),t("li",[a._v("金融、相关数据，选可靠性高的："),t("code",[a._v("file")])])])]),a._v(" "),t("li")])])])]),a._v(" "),t("li",[t("p",[t("code",[a._v("hdfs sink")])]),a._v(" "),t("ol",[t("li",[a._v("小文件\n"),t("ol",[t("li",[a._v("滚动数据：半小时")]),a._v(" "),t("li",[a._v("滚动大小：128m（块大小）")]),a._v(" "),t("li",[a._v("event 数量：0，表示禁用，每个event大小不一")])])])])]),a._v(" "),t("li",[t("p",[a._v("事务")]),a._v(" "),t("ol",[t("li",[t("code",[a._v("source")]),a._v(" -> "),t("code",[a._v("channel")]),a._v(" ：put 事务")]),a._v(" "),t("li",[t("code",[a._v("channel")]),a._v("-> "),t("code",[a._v("sink")]),a._v("：take事务")])])])]),a._v(" "),t("h3",{attrs:{id:"_2-三个器-拦截器、选择器、监控器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-三个器-拦截器、选择器、监控器"}},[a._v("#")]),a._v(" 2.三个器（拦截器、选择器、监控器）")]),a._v(" "),t("ol",[t("li",[a._v("flume 拦截器\n"),t("ol",[t("li",[a._v("eg\n"),t("ol",[t("li",[a._v("ETL拦截器：采集log 的 flume，用来轻度清洗 JSON 格式不完整的数据")]),a._v(" "),t("li",[a._v("时间拦截器：消费kafka 的 flume，解决零点漂移的问题")])])]),a._v(" "),t("li",[a._v("实现"),t("code",[a._v("Interceptor")]),a._v("，重写4个方法\n"),t("ol",[t("li",[a._v("初始化")]),a._v(" "),t("li",[a._v("单 event 处理：处理逻辑，fastjson 转成 jsonobject， try catch")]),a._v(" "),t("li",[a._v("多 event 处理")]),a._v(" "),t("li",[a._v("关闭")])])]),a._v(" "),t("li",[a._v("实现静态内部类 Builder")]),a._v(" "),t("li",[a._v("打成 jar 包 -> 上传到flume 的 lib 下 -> 配置文件指定 全类名￥Builder‘")])])]),a._v(" "),t("li",[a._v("flume channel 选择器\n"),t("ol",[t("li",[a._v("replicating：默认，一个 event 事件，发往所有的channel （广播）")]),a._v(" "),t("li",[a._v("multiplexing：一个 event，发往指定的channel （分流）")]),a._v(" "),t("li",[a._v("默认用replicating")])])]),a._v(" "),t("li",[a._v("flume 监控器\n"),t("ol",[t("li",[a._v("ganglia：\n"),t("ol",[t("li",[a._v("事务尝试提交的次数、事务成功提交的次数")]),a._v(" "),t("li",[a._v("如果两者差值较大，说明发生大量重试，有问题 --\x3e优化")])])])])])]),a._v(" "),t("h3",{attrs:{id:"_3-优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-优化"}},[a._v("#")]),a._v(" 3.优化")]),a._v(" "),t("ol",[t("li",[a._v("提高内存：默认不设置flume-env就是 20m，提高 4~6G（修改flume-env）")]),a._v(" "),t("li",[a._v("增加台数：重大节日提高增加日志服务器")]),a._v(" "),t("li",[a._v("filechannel：指定多目录（多磁盘）")])]),a._v(" "),t("h3",{attrs:{id:"_4-挂了"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-挂了"}},[a._v("#")]),a._v(" 4.挂了")]),a._v(" "),t("ol",[t("li",[a._v("尝试重启：")]),a._v(" "),t("li",[a._v("影响范围：\n"),t("ol",[t("li",[a._v("丢数：\n"),t("ol",[t("li",[t("code",[a._v("taildirsource")]),a._v(" 有断点续传")]),a._v(" "),t("li",[t("code",[a._v("channel")]),a._v("如果是 memory，会丢")]),a._v(" "),t("li",[a._v("事务")])])]),a._v(" "),t("li",[a._v("重复："),t("code",[a._v("taildirsource")]),a._v("可能重复")]),a._v(" "),t("li",[a._v("最终保障：日志服务器的日志文件，保存30天")])])]),a._v(" "),t("li",[a._v("定位问题：看日志")]),a._v(" "),t("li",[a._v("解决问题：eg：OOM -> 加内存")])]),a._v(" "),t("h2",{attrs:{id:"datax"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#datax"}},[a._v("#")]),a._v(" DataX")]),a._v(" "),t("h3",{attrs:{id:"_1-使用场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-使用场景"}},[a._v("#")]),a._v(" 1.使用场景")]),a._v(" "),t("ul",[t("li",[a._v("采集MySQL的数据")]),a._v(" "),t("li",[a._v("首次加载历史数据、全量同步策略的表（主要是维表，数据量小，除了用户表）")])]),a._v(" "),t("p",[a._v("​")]),a._v(" "),t("h3",{attrs:{id:"_2-遇到的问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-遇到的问题"}},[a._v("#")]),a._v(" 2.遇到的问题")]),a._v(" "),t("p",[a._v("1、空值问题")]),a._v(" "),t("p",[a._v("mysql\t\thive")]),a._v(" "),t("p",[a._v("null\t\t\t\\N")]),a._v(" "),t("p",[a._v("1）hive建表时，指定null的格式为''")]),a._v(" "),t("p",[a._v("2）改源码，增加 null 值转 \\N 的逻辑")]),a._v(" "),t("p",[a._v("​\t\t改 hdfswriter插件：代码增加了对 null 值的转换处理，如果是 NULL 值，转换成 \\N")]),a._v(" "),t("p",[a._v("​\t\t重新编译打包 hdfswriter 插件")]),a._v(" "),t("p",[a._v("​\t\t替换掉原来的 hdfswriter 插件")]),a._v(" "),t("h3",{attrs:{id:"_3-优化-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-优化-2"}},[a._v("#")]),a._v(" 3.优化")]),a._v(" "),t("p",[a._v("1、内存：4—8G，")]),a._v(" "),t("p",[a._v("2、并发：")]),a._v(" "),t("p",[a._v("调整 channel 数，3~5")]),a._v(" "),t("p",[a._v("3、channel 的速率：1M/S --\x3e 5M/S")]),a._v(" "),t("h3",{attrs:{id:"_4-几点开始跑-跑多久"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-几点开始跑-跑多久"}},[a._v("#")]),a._v(" 4.几点开始跑，跑多久")]),a._v(" "),t("p",[a._v("几个G（维表的全量：包含历史数据和增量数据）")]),a._v(" "),t("p",[a._v("每个的业务数据增量有多少？")]),a._v(" "),t("p",[a._v("​\t百万日活、10-20万订单对应10条数据，每条数据平均1K")]),a._v(" "),t("p",[a._v("​\t20万"),t("em",[a._v("10条")]),a._v("1K = 2G")]),a._v(" "),t("h3",{attrs:{id:"_5-datax有什么缺陷-与sqoop的区别"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-datax有什么缺陷-与sqoop的区别"}},[a._v("#")]),a._v(" 5.DataX有什么缺陷？与Sqoop的区别")]),a._v(" "),t("p",[a._v("Sqoop：已经停止维护")]),a._v(" "),t("p",[a._v("DataX：单进程多线程，单机的，不是分布式")]),a._v(" "),t("p",[a._v("数据量大时候，DataX  x1、x2、x3")]),a._v(" "),t("h2",{attrs:{id:"maxwell"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#maxwell"}},[a._v("#")]),a._v(" Maxwell")]),a._v(" "),t("h3",{attrs:{id:"_1-原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-原理"}},[a._v("#")]),a._v(" 1.原理")]),a._v(" "),t("p",[a._v("伪装成 MySQL 的从库，同步主库的 binlog")]),a._v(" "),t("p",[a._v("binlog 的解析格式：row 。。。")]),a._v(" "),t("h3",{attrs:{id:"_2-为什么要用它"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-为什么要用它"}},[a._v("#")]),a._v(" 2.为什么要用它")]),a._v(" "),t("p",[a._v("DataX不能做增量吗？---可以，通过时间字段筛选，create time、update time")]),a._v(" "),t("p",[a._v("未来实时要用，同步的是事实表，用户表")]),a._v(" "),t("h3",{attrs:{id:"_3-maxwell-和-canal-为什么用-maxwell"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-maxwell-和-canal-为什么用-maxwell"}},[a._v("#")]),a._v(" 3.Maxwell 和 Canal 为什么用 Maxwell")]),a._v(" "),t("p",[a._v("支持历史数据同步")]),a._v(" "),t("p",[a._v("格式更轻量")]),a._v(" "),t("p",[a._v("支持断点还原")]),a._v(" "),t("h3",{attrs:{id:"_4-maxwell-同步历史数据的时候-怎么保证一致性"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-maxwell-同步历史数据的时候-怎么保证一致性"}},[a._v("#")]),a._v(" 4.Maxwell 同步历史数据的时候，怎么保证一致性")]),a._v(" "),t("p",[a._v("只能保证至少一次（可能重复）")]),a._v(" "),t("p",[a._v("比如：同步历史数据的过程中，原表新增了数据，Bootstrap会拿到，Maxwell 监听 binlog 也会拿到")]),a._v(" "),t("h2",{attrs:{id:"sqoop"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sqoop"}},[a._v("#")]),a._v(" Sqoop")]),a._v(" "),t("h3",{attrs:{id:"_1-sqoop参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-sqoop参数"}},[a._v("#")]),a._v(" 1.Sqoop参数")]),a._v(" "),t("div",{staticClass:"language-sh extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sh"}},[t("code",[a._v("/opt/module/sqoop/bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--connect")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--username")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--password")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--target-dir "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--delete-target-dir "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--num-mappers "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--fields-terminated-by  "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--query")]),a._v("  "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"'),t("span",{pre:!0,attrs:{class:"token variable"}},[a._v("$2")]),a._v('"')]),a._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("' and $CONDITIONS;'")]),a._v("\n")])])]),t("h3",{attrs:{id:"_2-sqoop导入导出null存储一致性问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-sqoop导入导出null存储一致性问题"}},[a._v("#")]),a._v(" 2. Sqoop导入导出Null存储一致性问题")]),a._v(" "),t("p",[a._v("Hive中的Null在底层是以“\\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。")]),a._v(" "),t("h3",{attrs:{id:"_3-sqoop数据导出一致性问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-sqoop数据导出一致性问题"}},[a._v("#")]),a._v(" 3. Sqoop数据导出一致性问题")]),a._v(" "),t("p",[a._v("场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。")]),a._v(" "),t("p",[a._v("官网：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html")]),a._v(" "),t("p",[a._v("Since Sqoop breaks down export process into multiple transactions, it is possible that a failed export job may result in partial data being committed to the database. This can further lead to subsequent jobs failing due to insert collisions in some cases, or lead to duplicated data in others. You can overcome this problem by specifying a staging table via the --staging-table option which acts as an auxiliary table that is used to stage exported data. The staged data is finally moved to the destination table in a single transaction.")]),a._v(" "),t("p",[a._v("–staging-table方式")]),a._v(" "),t("p",[a._v('sqoop export --connect jdbc:mysql://192.168.137.10:3306/user_behavior --username root --password 123456 --table app_cource_study_report --columns watch_video_cnt,complete_video_cnt,dt --fields-terminated-by "\\t" --export-dir "/user/hive/warehouse/tmp.db/app_cource_study_analysis_${day}" --staging-table app_cource_study_report_tmp --clear-staging-table --input-null-string \'\\N\'')]),a._v(" "),t("h3",{attrs:{id:"_4-sqoop底层运行的任务是什么"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-sqoop底层运行的任务是什么"}},[a._v("#")]),a._v(" 4. Sqoop底层运行的任务是什么")]),a._v(" "),t("p",[a._v("只有Map阶段，没有Reduce阶段的任务。默认是4个MapTask。")]),a._v(" "),t("h3",{attrs:{id:"_5-sqoop一天导入多少数据"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-sqoop一天导入多少数据"}},[a._v("#")]),a._v(" 5. Sqoop一天导入多少数据")]),a._v(" "),t("p",[a._v("100万日活=》10万订单，1人10条，每天1g左右业务数据")]),a._v(" "),t("p",[a._v("Sqoop每天将1G的数据量导入到数仓。")]),a._v(" "),t("h3",{attrs:{id:"_6-sqoop数据导出的时候一次执行多长时间"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-sqoop数据导出的时候一次执行多长时间"}},[a._v("#")]),a._v(" 6. Sqoop数据导出的时候一次执行多长时间")]),a._v(" "),t("p",[a._v("每天晚上00:10开始执行，Sqoop任务一般情况20-30分钟的都有。取决于数据量（11.11，6.18等活动在1个小时左右）。")]),a._v(" "),t("h3",{attrs:{id:"_7-sqoop在导入数据的时候数据倾斜"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-sqoop在导入数据的时候数据倾斜"}},[a._v("#")]),a._v(" 7. Sqoop在导入数据的时候数据倾斜")]),a._v(" "),t("p",[a._v("Sqoop参数撇嘴： split-by：按照自增主键来切分表的工作单元。")]),a._v(" "),t("p",[a._v("num-mappers：启动N个map来并行导入数据，默认4个；")]),a._v(" "),t("h3",{attrs:{id:"_8-sqoop数据导出parquet-项目中遇到的问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_8-sqoop数据导出parquet-项目中遇到的问题"}},[a._v("#")]),a._v(" 8. Sqoop数据导出Parquet（项目中遇到的问题）")]),a._v(" "),t("p",[a._v("Ads层数据用Sqoop往MySql中导入数据的时候，如果用了orc（Parquet）不能导入，需转化成text格式")]),a._v(" "),t("p",[a._v("（1）创建临时表，把Parquet中表数据导入到临时表，把临时表导出到目标表用于可视化")]),a._v(" "),t("p",[a._v("（2）ads层建表的时候就不要建Parquet表")]),a._v(" "),t("h3",{attrs:{id:"举例说明"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#举例说明"}},[a._v("#")]),a._v(" 举例说明")]),a._v(" "),t("div",{staticClass:"language-sh extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sh"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#从hdfs上导入本地mysql中")]),a._v("\n\nsqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--connect")]),a._v(" jdbc:mysql://192.168.137.1/yd "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--username")]),a._v(" root "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-password")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("123456")]),a._v(" --export-dir /bigdata/hivedata/movie1/000000_0 "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-m")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--table")]),a._v(" m1 --fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'\\t'")]),a._v("\n\nsqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--connect")]),a._v(" jdbc:mysql://192.168.137.1/yd "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--username")]),a._v(" root "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-password")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("123456")]),a._v(" --export-dir /bigdata/hivedata/movie2/000000_0 "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-m")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--table")]),a._v(" m2 --fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'\\t'")]),a._v("\n\n\nsqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--connect")]),a._v(" jdbc:mysql://192.168.137.1/yd "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--username")]),a._v(" root "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-password")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("123456")]),a._v(" --export-dir /bigdata/hivedata/university1/000000_0 "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-m")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--table")]),a._v(" u1 --fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'\\t'")]),a._v("\n\nsqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--connect")]),a._v(" jdbc:mysql://192.168.137.1/yd "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--username")]),a._v(" root "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-password")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("123456")]),a._v(" --export-dir /bigdata/hivedata/university2/000000_0 "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-m")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--table")]),a._v(" u2 --fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'\\t'")]),a._v("\n\nsqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--connect")]),a._v(" jdbc:mysql://192.168.137.1/yd "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--username")]),a._v(" root "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-password")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("123456")]),a._v(" --export-dir /bigdata/hivedata/university3/000000_0 "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("-m")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[a._v("--table")]),a._v(" u3 --fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'\\t'")]),a._v("\n\n\n")])])])])}),[],!1,null,null,null);t.default=r.exports}}]);