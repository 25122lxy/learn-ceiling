<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Spark | Code Diary</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="/learn-ceiling/hand.jpg">
    <meta name="description" content="技术笔记、编程心得分享">
    <meta name="referrer" content="no-referrer">
    
    <link rel="preload" href="/learn-ceiling/assets/css/0.styles.df47c79f.css" as="style"><link rel="preload" href="/learn-ceiling/assets/js/app.235538a9.js" as="script"><link rel="preload" href="/learn-ceiling/assets/js/2.c7d2b1f7.js" as="script"><link rel="preload" href="/learn-ceiling/assets/js/21.f4e6e20a.js" as="script"><link rel="prefetch" href="/learn-ceiling/assets/js/10.c4c4f495.js"><link rel="prefetch" href="/learn-ceiling/assets/js/11.06c1692d.js"><link rel="prefetch" href="/learn-ceiling/assets/js/12.e70990f1.js"><link rel="prefetch" href="/learn-ceiling/assets/js/13.5ea6f405.js"><link rel="prefetch" href="/learn-ceiling/assets/js/14.5ce76ccb.js"><link rel="prefetch" href="/learn-ceiling/assets/js/15.b5c61832.js"><link rel="prefetch" href="/learn-ceiling/assets/js/16.6e4e0759.js"><link rel="prefetch" href="/learn-ceiling/assets/js/17.b6f52e7f.js"><link rel="prefetch" href="/learn-ceiling/assets/js/18.155f2caf.js"><link rel="prefetch" href="/learn-ceiling/assets/js/19.ea21c3cf.js"><link rel="prefetch" href="/learn-ceiling/assets/js/20.33c43e06.js"><link rel="prefetch" href="/learn-ceiling/assets/js/22.485d0a07.js"><link rel="prefetch" href="/learn-ceiling/assets/js/23.fb805a45.js"><link rel="prefetch" href="/learn-ceiling/assets/js/24.6a258cf5.js"><link rel="prefetch" href="/learn-ceiling/assets/js/25.c4c4023d.js"><link rel="prefetch" href="/learn-ceiling/assets/js/26.d92cf884.js"><link rel="prefetch" href="/learn-ceiling/assets/js/27.5d88d619.js"><link rel="prefetch" href="/learn-ceiling/assets/js/28.a0b76e62.js"><link rel="prefetch" href="/learn-ceiling/assets/js/29.3f0426da.js"><link rel="prefetch" href="/learn-ceiling/assets/js/3.d1fbc4fe.js"><link rel="prefetch" href="/learn-ceiling/assets/js/30.7adccd2c.js"><link rel="prefetch" href="/learn-ceiling/assets/js/31.7d0b9a7f.js"><link rel="prefetch" href="/learn-ceiling/assets/js/32.6970bcc0.js"><link rel="prefetch" href="/learn-ceiling/assets/js/33.eeb35d50.js"><link rel="prefetch" href="/learn-ceiling/assets/js/34.ea24ea8b.js"><link rel="prefetch" href="/learn-ceiling/assets/js/35.c40a585a.js"><link rel="prefetch" href="/learn-ceiling/assets/js/36.79790a12.js"><link rel="prefetch" href="/learn-ceiling/assets/js/37.a847b8a1.js"><link rel="prefetch" href="/learn-ceiling/assets/js/38.9a6ab34d.js"><link rel="prefetch" href="/learn-ceiling/assets/js/39.bf0bc1a7.js"><link rel="prefetch" href="/learn-ceiling/assets/js/4.b6acf418.js"><link rel="prefetch" href="/learn-ceiling/assets/js/40.2e93acfd.js"><link rel="prefetch" href="/learn-ceiling/assets/js/41.bcc213e5.js"><link rel="prefetch" href="/learn-ceiling/assets/js/42.558394fc.js"><link rel="prefetch" href="/learn-ceiling/assets/js/43.cd846378.js"><link rel="prefetch" href="/learn-ceiling/assets/js/44.afcb9063.js"><link rel="prefetch" href="/learn-ceiling/assets/js/45.526cdab6.js"><link rel="prefetch" href="/learn-ceiling/assets/js/46.3bbeeda3.js"><link rel="prefetch" href="/learn-ceiling/assets/js/47.dba7847e.js"><link rel="prefetch" href="/learn-ceiling/assets/js/48.29379048.js"><link rel="prefetch" href="/learn-ceiling/assets/js/49.f526a58a.js"><link rel="prefetch" href="/learn-ceiling/assets/js/5.3747e8f8.js"><link rel="prefetch" href="/learn-ceiling/assets/js/50.2be0a3da.js"><link rel="prefetch" href="/learn-ceiling/assets/js/51.46ccf051.js"><link rel="prefetch" href="/learn-ceiling/assets/js/52.9bcd0722.js"><link rel="prefetch" href="/learn-ceiling/assets/js/53.06fef668.js"><link rel="prefetch" href="/learn-ceiling/assets/js/54.534da4f1.js"><link rel="prefetch" href="/learn-ceiling/assets/js/55.22f59a2a.js"><link rel="prefetch" href="/learn-ceiling/assets/js/56.5b1e2aa6.js"><link rel="prefetch" href="/learn-ceiling/assets/js/57.daacfc08.js"><link rel="prefetch" href="/learn-ceiling/assets/js/58.cb240494.js"><link rel="prefetch" href="/learn-ceiling/assets/js/59.f0cab42c.js"><link rel="prefetch" href="/learn-ceiling/assets/js/6.3688e4ed.js"><link rel="prefetch" href="/learn-ceiling/assets/js/60.74872728.js"><link rel="prefetch" href="/learn-ceiling/assets/js/61.a7a19f44.js"><link rel="prefetch" href="/learn-ceiling/assets/js/62.bda341fd.js"><link rel="prefetch" href="/learn-ceiling/assets/js/63.39847a02.js"><link rel="prefetch" href="/learn-ceiling/assets/js/64.8ac906c5.js"><link rel="prefetch" href="/learn-ceiling/assets/js/7.dddd7af2.js"><link rel="prefetch" href="/learn-ceiling/assets/js/8.4b31d8be.js"><link rel="prefetch" href="/learn-ceiling/assets/js/9.9a27c3e2.js">
    <link rel="stylesheet" href="/learn-ceiling/assets/css/0.styles.df47c79f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><main><main slot="sidebar-top"><a href="https://github.com/25122lxy/" aria-label="View source on GitHub" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path> <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" class="octo-arm" style="transform-origin: 130px 106px;"></path> <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a></main> <div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/learn-ceiling/" class="home-link router-link-active"><!----> <span class="site-name">Code Diary</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/learn-ceiling/" class="nav-link">
  Index
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Learn" class="dropdown-title"><span class="title">Learn</span> <span class="arrow down"></span></button> <button type="button" aria-label="Learn" class="mobile-dropdown-title"><span class="title">Learn</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/Microservice/01-初识微服务.html" class="nav-link">
  Microservice
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/RabbitMQ/1.RabbitMQ-简介及安装.html" class="nav-link">
  RabbitMQ
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/Redis/0-Redis-Basic.html" class="nav-link">
  Redis
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/SQL/SQL案例.html" class="nav-link">
  MySQL
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/前端快速入门/01-HTML-CSS.html" class="nav-link">
  前端快速入门
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/Linux/01-Linux入门.html" class="nav-link">
  Linux
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Project" class="dropdown-title"><span class="title">Project</span> <span class="arrow down"></span></button> <button type="button" aria-label="Project" class="mobile-dropdown-title"><span class="title">Project</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/RabbitMQ/1.RabbitMQ-简介及安装.html" class="nav-link">
  社区精品汇
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Interview" class="dropdown-title"><span class="title">Interview</span> <span class="arrow down"></span></button> <button type="button" aria-label="Interview" class="mobile-dropdown-title"><span class="title">Interview</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/00-查漏补缺.html" class="nav-link">
  查漏补缺
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/01-Java基础.html" class="nav-link">
  Java基础
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/02-Redis.html" class="nav-link">
  Redis
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/03-MySQL.html" class="nav-link">
  MySQL
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/04-Java框架.html" class="nav-link">
  Java框架
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/05-微服务.html" class="nav-link">
  微服务
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/06-消息中间件.html" class="nav-link">
  消息中间件
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/07-Java集合.html" class="nav-link">
  Java集合
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/08-Java并发.html" class="nav-link">
  Java并发
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/09-JVM.html" class="nav-link">
  JVM
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/10-Linux.html" class="nav-link">
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/11-Hadoop.html" class="nav-link">
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/12-Hive.html" class="nav-link">
  Hive
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/13-Spark.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Spark
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/16-数据采集工具.html" class="nav-link">
  数据采集工具
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="lxy25122-blog" class="dropdown-title"><span class="title">lxy25122-blog</span> <span class="arrow down"></span></button> <button type="button" aria-label="lxy25122-blog" class="mobile-dropdown-title"><span class="title">lxy25122-blog</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lxy25122" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/tjlxy" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/learn-ceiling/" class="nav-link">
  Index
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Learn" class="dropdown-title"><span class="title">Learn</span> <span class="arrow down"></span></button> <button type="button" aria-label="Learn" class="mobile-dropdown-title"><span class="title">Learn</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/Microservice/01-初识微服务.html" class="nav-link">
  Microservice
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/RabbitMQ/1.RabbitMQ-简介及安装.html" class="nav-link">
  RabbitMQ
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/Redis/0-Redis-Basic.html" class="nav-link">
  Redis
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/SQL/SQL案例.html" class="nav-link">
  MySQL
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/前端快速入门/01-HTML-CSS.html" class="nav-link">
  前端快速入门
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/Linux/01-Linux入门.html" class="nav-link">
  Linux
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Project" class="dropdown-title"><span class="title">Project</span> <span class="arrow down"></span></button> <button type="button" aria-label="Project" class="mobile-dropdown-title"><span class="title">Project</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Learn/RabbitMQ/1.RabbitMQ-简介及安装.html" class="nav-link">
  社区精品汇
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Interview" class="dropdown-title"><span class="title">Interview</span> <span class="arrow down"></span></button> <button type="button" aria-label="Interview" class="mobile-dropdown-title"><span class="title">Interview</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/00-查漏补缺.html" class="nav-link">
  查漏补缺
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/01-Java基础.html" class="nav-link">
  Java基础
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/02-Redis.html" class="nav-link">
  Redis
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/03-MySQL.html" class="nav-link">
  MySQL
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/04-Java框架.html" class="nav-link">
  Java框架
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/05-微服务.html" class="nav-link">
  微服务
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/06-消息中间件.html" class="nav-link">
  消息中间件
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/07-Java集合.html" class="nav-link">
  Java集合
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/08-Java并发.html" class="nav-link">
  Java并发
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/09-JVM.html" class="nav-link">
  JVM
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/10-Linux.html" class="nav-link">
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/11-Hadoop.html" class="nav-link">
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/12-Hive.html" class="nav-link">
  Hive
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/13-Spark.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Spark
</a></li><li class="dropdown-item"><!----> <a href="/learn-ceiling/handbook/Interview/16-数据采集工具.html" class="nav-link">
  数据采集工具
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="lxy25122-blog" class="dropdown-title"><span class="title">lxy25122-blog</span> <span class="arrow down"></span></button> <button type="button" aria-label="lxy25122-blog" class="mobile-dropdown-title"><span class="title">lxy25122-blog</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lxy25122" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/tjlxy" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/learn-ceiling/" aria-current="page" class="sidebar-link">学前必读</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Learn</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Project</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Interview</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/learn-ceiling/handbook/Interview/00-查漏补缺.html" class="sidebar-link">查漏补缺</a></li><li><a href="/learn-ceiling/handbook/Interview/01-Java基础.html" class="sidebar-link">Java基础</a></li><li><a href="/learn-ceiling/handbook/Interview/02-Redis.html" class="sidebar-link">Redis</a></li><li><a href="/learn-ceiling/handbook/Interview/03-MySQL.html" class="sidebar-link">MySQL</a></li><li><a href="/learn-ceiling/handbook/Interview/04-Java框架.html" class="sidebar-link">Java框架</a></li><li><a href="/learn-ceiling/handbook/Interview/05-微服务.html" class="sidebar-link">微服务</a></li><li><a href="/learn-ceiling/handbook/Interview/06-消息中间件.html" class="sidebar-link">消息中间件</a></li><li><a href="/learn-ceiling/handbook/Interview/07-Java集合.html" class="sidebar-link">Java集合</a></li><li><a href="/learn-ceiling/handbook/Interview/08-Java并发.html" class="sidebar-link">Java并发</a></li><li><a href="/learn-ceiling/handbook/Interview/09-JVM.html" class="sidebar-link">JVM</a></li><li><a href="/learn-ceiling/handbook/Interview/10-Linux.html" class="sidebar-link">Linux</a></li><li><a href="/learn-ceiling/handbook/Interview/11-Hadoop.html" class="sidebar-link">Hadoop</a></li><li><a href="/learn-ceiling/handbook/Interview/12-Hive.html" class="sidebar-link">Hive</a></li><li><a href="/learn-ceiling/handbook/Interview/13-Spark.html" aria-current="page" class="active sidebar-link">Spark</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#spark入门" class="sidebar-link">Spark入门</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_1-spark的运行流程✔" class="sidebar-link">1.Spark的运行流程✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_2-spark有哪些组件" class="sidebar-link">2.Spark有哪些组件</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#spark的使用场景✔" class="sidebar-link">Spark的使用场景✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_33-spark的有几种部署模式-每种模式特点✔" class="sidebar-link">33.spark的有几种部署模式，每种模式特点✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_104-spark-常用端口号✔" class="sidebar-link">104.Spark 常用端口号✔</a></li></ul></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#sparkcore" class="sidebar-link">SparkCore</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_3-spark中rdd机制理解吗✔" class="sidebar-link">3.Spark中RDD机制理解吗✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_4-rdd中reducebykey与groupbykey那个性能好-为什么✔" class="sidebar-link">4.RDD中reduceByKey与groupByKey那个性能好，为什么✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_5-介绍一下cogrouprdd实现原理-你再什么场景下用过这个rdd" class="sidebar-link">5.介绍一下cogroupRdd实现原理，你再什么场景下用过这个rdd</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_6-如何区分rdd的宽窄依赖✔" class="sidebar-link">6.如何区分RDD的宽窄依赖✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_7-为什么要设计宽窄依赖" class="sidebar-link">7.为什么要设计宽窄依赖</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_8-dag是什么-localhost-4040-✔" class="sidebar-link">8.DAG是什么（localhost:4040）✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_9-dag中为什么要划分stage" class="sidebar-link">9.DAG中为什么要划分Stage</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_10-如何划分dag的stage" class="sidebar-link">10.如何划分DAG的stage</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_11-spark中的oom的问题" class="sidebar-link">11.Spark中的OOM的问题</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_12-spark中数据的位置被谁管理的" class="sidebar-link">12.Spark中数据的位置被谁管理的</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_13-spark程序执行-有时候默认为什么会产生很多task-怎么修改默认task执行个数" class="sidebar-link">13.Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_14-spark与mapreduce的shuffle的区别" class="sidebar-link">14.Spark与MapReduce的Shuffle的区别</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_17-通常来说-spark与mapreduce相比-spark运行效率更高。请说明效率更高来源于spark内置的哪些机制" class="sidebar-link">17.通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_18-hadoop和spark的相同点和不同点" class="sidebar-link">18.Hadoop和Spark的相同点和不同点</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_19-hadoop和spark使用场景" class="sidebar-link">19.Hadoop和Spark使用场景</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_20-spark如何保证宕机迅速恢复" class="sidebar-link">20.Spark如何保证宕机迅速恢复</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_21-rdd持久化-缓存-原理✔" class="sidebar-link">21.RDD持久化（缓存）原理✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_22-checkpoint检查点机制" class="sidebar-link">22.Checkpoint检查点机制</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#缓存和检查点区别✔" class="sidebar-link">缓存和检查点区别✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_23-checkpoint和持久化机制的区别" class="sidebar-link">23.Checkpoint和持久化机制的区别</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_32-rdd有哪些缺陷" class="sidebar-link">32.RDD有哪些缺陷？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_34-spark为什么比mapreduce快" class="sidebar-link">34.Spark为什么比mapreduce快</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_35-数据本地性是在哪个环节确定的" class="sidebar-link">35.数据本地性是在哪个环节确定的</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_36-rdd的弹性表现在哪几点" class="sidebar-link">36.RDD的弹性表现在哪几点</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_37-spark的数据本地性有哪几种" class="sidebar-link">37. Spark的数据本地性有哪几种</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_38-spark为什么要持久化-一般什么场景下要进行persist操作" class="sidebar-link">38.Spark为什么要持久化，一般什么场景下要进行persist操作</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_39-描述yarn执行一个任务的过程-☆☆☆☆☆-【hadoop" class="sidebar-link">39.描述Yarn执行一个任务的过程？（☆☆☆☆☆）【hadoop</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_40-spark-on-yarn-模式有哪些优点-☆☆☆☆☆-√" class="sidebar-link">40.Spark on Yarn 模式有哪些优点？（☆☆☆☆☆）√</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_41-谈谈你对container的理解-☆☆☆☆☆" class="sidebar-link">41.谈谈你对container的理解？（☆☆☆☆☆）</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_42-spark使用parquet文件存储格式能带来哪些好处-☆☆☆☆☆" class="sidebar-link">42.Spark使用parquet文件存储格式能带来哪些好处？（☆☆☆☆☆）</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_43-介绍parition和block有什么关联关系-☆☆☆☆☆" class="sidebar-link">43.介绍parition和block有什么关联关系？（☆☆☆☆☆）</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_44-不需要排序的hash-shuffle是否一定比需要排序的sort-shuffle速度快" class="sidebar-link">44.不需要排序的hash shuffle是否一定比需要排序的sort shuffle速度快？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_45-sort-based-shuffle的缺陷-☆☆☆☆☆" class="sidebar-link">45.Sort-based shuffle的缺陷? （☆☆☆☆☆）</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_46-spark-storage-memoryfraction参数的含义-实际生产中如何调优" class="sidebar-link">46.spark.storage.memoryFraction参数的含义,实际生产中如何调优？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_47-介绍一下你对unified-memory-management内存管理模型的理解-☆☆☆☆☆" class="sidebar-link">47.介绍一下你对Unified Memory Management内存管理模型的理解？（☆☆☆☆☆）</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_48-spark有哪两种算子✔" class="sidebar-link">48.Spark有哪两种算子✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_49-spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子" class="sidebar-link">49.Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_50-rdd创建有哪几种方式" class="sidebar-link">50.RDD创建有哪几种方式？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_51-spark并行度怎么设置比较合适" class="sidebar-link">51.Spark并行度怎么设置比较合适？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_52-spark如何处理不能被序列化的对象" class="sidebar-link">52.Spark如何处理不能被序列化的对象？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_53-collect功能是什么-其底层是怎么实现的" class="sidebar-link">53.collect功能是什么，其底层是怎么实现的？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_54-为什么spark-application在没有获得足够的资源-job就开始执行了-可能会导致什么问题发生" class="sidebar-link">54.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么问题发生？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_55-map与flatmap的区别" class="sidebar-link">55.map与flatMap的区别？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_56-spark-on-mesos中-什么是的粗粒度分配-什么是细粒度分配-各自的优点和缺点是什么" class="sidebar-link">56.Spark on Mesos中，什么是的粗粒度分配，什么是细粒度分配，各自的优点和缺点是什么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_57-driver的功能是什么" class="sidebar-link">57.driver的功能是什么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_58-spark技术栈有哪些组件-每个组件都有什么功能-适合什么应用场景" class="sidebar-link">58.Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_59-spark中worker的主要工作是什么" class="sidebar-link">59.Spark中Worker的主要工作是什么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_60-cache和pesist的区别✔" class="sidebar-link">60.cache和pesist的区别✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_61-cache后面能不能接其他算子-它是不是action操作" class="sidebar-link">61.cache后面能不能接其他算子,它是不是action操作</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_62-reducebykey是不是action" class="sidebar-link">62.reduceByKey是不是action？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_63-rdd通过linage-记录数据更新-的方式为何很高效" class="sidebar-link">63.RDD通过Linage（记录数据更新）的方式为何很高效？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_64-为什么要进行序列化" class="sidebar-link">64.为什么要进行序列化？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#kryo序列化" class="sidebar-link">Kryo序列化</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_65-yarn中的container是由谁负责销毁的-在hadoop-mapreduce中container可以复用么" class="sidebar-link">65. Yarn中的container是由谁负责销毁的，在Hadoop Mapreduce中container可以复用么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_66-提交任务时-如何指定spark-application的运行模式" class="sidebar-link">66.提交任务时，如何指定Spark Application的运行模式？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_67-不启动spark集群master和work服务-可不可以运行spark程序" class="sidebar-link">67.不启动Spark集群Master和work服务，可不可以运行Spark程序？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_68-spark-on-yarn-cluster-模式下-applicationmaster和driver是在同一个进程么" class="sidebar-link">68.spark on yarn Cluster 模式下，ApplicationMaster和driver是在同一个进程么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_69-运行在yarn中application有几种类型的container" class="sidebar-link">69.运行在yarn中Application有几种类型的container？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_70-executor启动时-资源通过哪几个参数指定" class="sidebar-link">70.Executor启动时，资源通过哪几个参数指定？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_71-导致executor产生full-gc-的原因-可能导致什么问题" class="sidebar-link">71.导致Executor产生FULL gc 的原因，可能导致什么问题？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_72-spark累加器有哪些特点" class="sidebar-link">72.Spark累加器有哪些特点？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_73-spark-hashparitioner的弊端是什么" class="sidebar-link">73.spark hashParitioner的弊端是什么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_74-rangepartitioner分区的原理" class="sidebar-link">74.RangePartitioner分区的原理？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_75-rangepartioner分区器特点" class="sidebar-link">75.rangePartioner分区器特点？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_76-如何理解standalone模式下-spark资源分配是粗粒度的" class="sidebar-link">76.如何理解Standalone模式下，Spark资源分配是粗粒度的？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_77-union操作是产生宽依赖还是窄依赖" class="sidebar-link">77.union操作是产生宽依赖还是窄依赖？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_78-窄依赖父rdd的partition和子rdd的parition是不是都是一对一的关系" class="sidebar-link">78.窄依赖父RDD的partition和子RDD的parition是不是都是一对一的关系？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_79-hadoop中-mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子" class="sidebar-link">79.Hadoop中，Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_80-什么是shuffle-以及为什么需要shuffle" class="sidebar-link">80.什么是shuffle，以及为什么需要shuffle？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_81-spark中的hashshufle的有哪些不足" class="sidebar-link">81.Spark中的HashShufle的有哪些不足？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_82-conslidate是如何优化hash-shuffle时在map端产生的小文件" class="sidebar-link">82.conslidate是如何优化Hash shuffle时在map端产生的小文件？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_83-spark-default-parallelism这个参数有什么意义-实际生产中如何设置" class="sidebar-link">83.spark.default.parallelism这个参数有什么意义，实际生产中如何设置？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_84-spark-shuffle-memoryfraction参数的含义-以及优化经验" class="sidebar-link">84.spark.shuffle.memoryFraction参数的含义，以及优化经验？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_85-spark中standalone模式特点-有哪些优点和缺点" class="sidebar-link">85.Spark中standalone模式特点，有哪些优点和缺点？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_86-常见的数压缩方式-你们生产集群采用了什么压缩方式-提升了多少效率" class="sidebar-link">86.常见的数压缩方式，你们生产集群采用了什么压缩方式，提升了多少效率？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_87-使用scala代码实现wordcount" class="sidebar-link">87.使用scala代码实现WordCount？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_89-rdd的数据结构是怎么样的✔" class="sidebar-link">89.RDD的数据结构是怎么样的✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_90-rdd算子里操作一个外部map-比如往里面put数据-然后算子外再遍历map-会有什么问题吗" class="sidebar-link">90.RDD算子里操作一个外部map，比如往里面put数据，然后算子外再遍历map，会有什么问题吗？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_91-说说你对hadoop生态的认识。" class="sidebar-link">91.说说你对Hadoop生态的认识。</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_92-hbase-region多大会分区-spark读取hbase数据是如何划分partition的" class="sidebar-link">92.hbase region多大会分区，spark读取hbase数据是如何划分partition的？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_93-spark数据倾斜" class="sidebar-link">93.Spark数据倾斜</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_94-spark中taskschedule的作用是啥" class="sidebar-link">94.Spark中TaskSchedule的作用是啥</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_95-spark的内存分布" class="sidebar-link">95.Spark的内存分布</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_96-spark处理小文件" class="sidebar-link">96.Spark处理小文件</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_97-spark如何实现报警功能" class="sidebar-link">97.spark如何实现报警功能</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_98-spark的shuffle✔" class="sidebar-link">98.Spark的shuffle✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#哪些算子会引起shuffle✔" class="sidebar-link">哪些算子会引起shuffle✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_99-spark中oom" class="sidebar-link">99.Spark中OOM</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_100-reparation和coalesce关系与区别✔" class="sidebar-link">100.Reparation和Coalesce关系与区别✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_101-yarn-client的提交流程" class="sidebar-link">101.Yarn-client的提交流程</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_102-yarn-cluster的提交流程" class="sidebar-link">102.Yarn-cluster的提交流程</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_103-spark调优原则✔" class="sidebar-link">103.Spark调优原则✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_105-map和mappartitions区别✔" class="sidebar-link">105.map和mapPartitions区别✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_106-reducebykey、foldbykey、aggregatebykey、combinebykey区别✔" class="sidebar-link">106. reduceByKey、foldByKey、aggregateByKey、combineByKey区别✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_107-spark任务的划分✔" class="sidebar-link">107.Spark任务的划分✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_108-cache缓存级别✔" class="sidebar-link">108.cache缓存级别✔</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_109-spark分区" class="sidebar-link">109.Spark分区</a></li></ul></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#sparksql" class="sidebar-link">SparkSQL</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_15-sparksql执行的流程" class="sidebar-link">15.SparkSql执行的流程</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_16-saprksql是如何将数据写到hive表的" class="sidebar-link">16.SaprkSql是如何将数据写到hive表的</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_88-spark-sql为什么比hive快呢" class="sidebar-link">88.Spark sql为什么比hive快呢？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_110-sparksql中rdd、dataframe、dataset三者的转换-笔试重点" class="sidebar-link">110.SparkSQL中RDD、DataFrame、DataSet三者的转换（笔试重点）</a></li></ul></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#sparkstreaming" class="sidebar-link">SparkStreaming</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_24-sparkstreaming以及基本工作原理" class="sidebar-link">24.SparkStreaming以及基本工作原理？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_25-dstream以及基本工作原理" class="sidebar-link">25.DStream以及基本工作原理？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_26-sparkstreaming整合kafka的两种模式" class="sidebar-link">26.SparkStreaming整合Kafka的两种模式？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_27-spark主备切换原理知道吗" class="sidebar-link">27.Spark主备切换原理知道吗？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_28-spark解决了hadoop的哪些问题" class="sidebar-link">28.Spark解决了Hadoop的哪些问题？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_29-sparkmasterha主从切换过程会不会影响到集群已有的作业的运行-为什么" class="sidebar-link">29.SparkMasterHA主从切换过程会不会影响到集群已有的作业的运行，为什么？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_30-sparkmaster使用zookeeper进行ha-有哪些源数据保存到zookeeper里面" class="sidebar-link">30.SparkMaster使用Zookeeper进行HA,有哪些源数据保存到Zookeeper里面？</a></li><li class="sidebar-sub-header"><a href="/learn-ceiling/handbook/Interview/13-Spark.html#_31-如何实现sparkstreaming读取flume中的数据" class="sidebar-link">31.如何实现SparkStreaming读取Flume中的数据？</a></li></ul></li></ul></li><li><a href="/learn-ceiling/handbook/Interview/16-数据采集工具.html" class="sidebar-link">数据采集工具</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="spark"><a href="#spark" class="header-anchor">#</a> Spark</h1> <h2 id="spark入门"><a href="#spark入门" class="header-anchor">#</a> Spark入门</h2> <h3 id="_1-spark的运行流程✔"><a href="#_1-spark的运行流程✔" class="header-anchor">#</a> 1.Spark的运行流程✔</h3> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/29617954/1673686098022-0ad1a544-3cc9-468a-a73b-35d659359eab.png#averageHue=%23f6eadc&amp;clientId=u3b80b119-e69c-4&amp;from=paste&amp;height=365&amp;id=u2ea949d8&amp;originHeight=456&amp;originWidth=713&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=66950&amp;status=done&amp;style=none&amp;taskId=ub037ff62-02d1-4702-84f8-a512c6da568&amp;title=&amp;width=570.4" alt="image.png"></p> <ol><li>SparkContext向资源管理器注册并向资源管理器申请运行Executor</li> <li>资源管理器分配Executor，然后资源管理器启动Executor</li> <li>Executor发送心跳至资源管理器</li> <li>SparkContext构建DAG有向无环图</li> <li>将DAG分解成Stage（TaskSet）</li> <li>把Stage发送给TaskScheduler</li> <li>Executor向SparkContext申请Task</li> <li>TaskScheduler将Task发送给Executor运行</li> <li>同时SparkContext将应用程序代码发放给Executor</li> <li>Task在Executor上运行，运行完毕释放所有资源</li></ol> <h3 id="_2-spark有哪些组件"><a href="#_2-spark有哪些组件" class="header-anchor">#</a> 2.Spark有哪些组件</h3> <ul><li>master：管理集群和节点，不参与计算。</li> <li>worker：计算节点，进程本身不参与计算，和master汇报。</li> <li>driver：运行程序的main方法，创建sparkcontext对象。</li> <li>sparkcontext：控制整个application的生命周期，包括dagsheduler和taskscheduler等组件。</li> <li>clinet：用户提交程序的入口。</li></ul> <h3 id="spark的使用场景✔"><a href="#spark的使用场景✔" class="header-anchor">#</a> Spark的使用场景✔</h3> <ul><li><p>hive on spark</p> <p>只把它当作引擎，写的还是hivesql，编译器、解析器、都是hive，只由优化器、执行器是spark</p></li> <li><p>spark on hive</p> <p>sparksql 去读写 hive 表，只用到了 hive 的元数据，其他编译、解析、优化、执行都是spark自己的</p></li></ul> <h3 id="_33-spark的有几种部署模式-每种模式特点✔"><a href="#_33-spark的有几种部署模式-每种模式特点✔" class="header-anchor">#</a> 33.spark的有几种部署模式，每种模式特点✔</h3> <ul><li><p>local，测试</p> <ul><li>将 Spark 应用以多线程的方式之间运行在本地（local:启动一个executor。local[k]:启动k歌executor。local[*]：启动跟CPU数目相同的executor）</li></ul></li> <li><p>standalone，自己管资源</p> <ul><li>分布式部署集群，自带完整服务，资源管理和任务监控是Spark自己监控</li></ul></li> <li><p><strong>yarn</strong>，yarn管资源</p> <ul><li>client，driver在提交节点启动，适合生产</li> <li>cluster，driver由yarn决定在哪启动，适合调试</li></ul></li> <li><p>mesos，国外用</p></li> <li><p>k8s，未来趋势</p></li></ul> <h3 id="_104-spark-常用端口号✔"><a href="#_104-spark-常用端口号✔" class="header-anchor">#</a> 104.Spark 常用端口号✔</h3> <p><strong>1）4040 spark-shell任务端口</strong>
2）7077 内部通讯端口。类比Hadoop的8020
3）8080 查看任务执行情况端口。类比Hadoop的8088
<strong>4）18080 历史服务器。类比Hadoop的19888</strong>
注意：由于Spark只负责计算，所有并没有Hadoop中存储数据的端口50070</p> <h2 id="sparkcore"><a href="#sparkcore" class="header-anchor">#</a> SparkCore</h2> <h3 id="_3-spark中rdd机制理解吗✔"><a href="#_3-spark中rdd机制理解吗✔" class="header-anchor">#</a> 3.Spark中RDD机制理解吗✔</h3> <ul><li>数据结构、hdfs文件、集合</li> <li>分布式弹性数据集</li></ul> <p><strong>rdd分布式弹性数据集，简单的理解成一种数据结构</strong>，是spark框架上的通用货币。所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。<strong>RDD是spark提供的核心抽象，全称为弹性分布式数据集</strong>。</p> <p><strong>RDD在逻辑上是一个hdfs文件，在抽象上是一种元素集合，包含了数据。它是被分区的，分为多个分区，每个分区分 布在集群中的不同结点上，从而让RDD中的数据可以被并行操作（分布式数据集）</strong></p> <p>比如有个RDD有90W数据，3个partition，则每个分区上有30W数据。RDD通常通过Hadoop上的文件，即HDFS或 者HIVE表来创建，还可以通过应用程序中的集合来创建；<strong>RDD最重要的特性就是容错性，可以自动从节点失败中恢复过来</strong>。即如果某个结点上的RDDpartition因为节点故障，导致数据丢失，那么RDD可以通过自己的数据来源重新计算该 partition。这一切对使用者都是透明的。</p> <p><strong>RDD的数据默认存放在内存中，但是当内存资源不足时，spark会自动将RDD数据写入磁盘</strong>。比如某结点内存只能处理20W数据，那么这20W数据就会放入内存中计算，剩下10W放到磁盘中。<strong>RDD的弹性体现在于RDD上自动进行内存和磁盘之间权衡和切换的机制</strong>。</p> <h3 id="_4-rdd中reducebykey与groupbykey那个性能好-为什么✔"><a href="#_4-rdd中reducebykey与groupbykey那个性能好-为什么✔" class="header-anchor">#</a> 4.RDD中reduceByKey与groupByKey那个性能好，为什么✔</h3> <ul><li><strong>reduceByKey</strong>发生在Map阶段，具有预聚合操作，既对数据重分区，也根据reduce 逻辑聚合，会shuffle</li> <li>groupByKey发生在Reduce阶段，具有预聚合操作，只对数据重分区，不计算，会shuffle</li></ul> <p><strong>reduceByKey</strong>：reduceByKey会在结果发送至reducer之前会对每个mapper在本地进行merge，有点类似于在 MapReduce中的combiner。这样做的好处在于，在map端进行一次reduce之后，数据量会大幅度减小，从而减小传输， 保证reduce端能够更快的进行结果计算。</p> <p><strong>groupByKey</strong>：groupByKey会对每一个RDD中的value值进行聚合形成一个序列(Iterator)，此操作发生在reduce端，所以 势必会将所有的数据通过网络进行传输，造成不必要的浪费。同时如果数据量十分大，可能还会造成 OutOfMemoryError。所以在进行大量数据的reduce操作时候建议使用reduceByKey。不仅可以提高速度，还可以防止使 用groupByKey造成的内存溢出问题。</p> <h3 id="_5-介绍一下cogrouprdd实现原理-你再什么场景下用过这个rdd"><a href="#_5-介绍一下cogrouprdd实现原理-你再什么场景下用过这个rdd" class="header-anchor">#</a> 5.介绍一下cogroupRdd实现原理，你再什么场景下用过这个rdd</h3> <ul><li><p>cogroupRdd（合并RDD）</p> <ul><li>对多个（2~4）RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合。</li> <li>与reduceByKey不同的是：reduceByKey针对一个RDD中相同的key进行合并。而<strong>cogroup针对多个RDD中相同的key的 元素进行合并。</strong></li></ul></li> <li><p>使用场景：表关联查询或处理重复的key</p></li></ul> <h3 id="_6-如何区分rdd的宽窄依赖✔"><a href="#_6-如何区分rdd的宽窄依赖✔" class="header-anchor">#</a> 6.如何区分RDD的宽窄依赖✔</h3> <p>窄依赖：父RDD的一个分区只会被子RDD的一个分区依赖；</p> <p>宽依赖：父RDD的一个分区会被子RDD的多个分区依赖(<strong>涉及到shuffle</strong>)。</p> <h3 id="_7-为什么要设计宽窄依赖"><a href="#_7-为什么要设计宽窄依赖" class="header-anchor">#</a> 7.为什么要设计宽窄依赖</h3> <p>对于窄依赖： 窄依赖的多个分区可以并行计算； 窄依赖的一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了。</p> <p>对于宽依赖： 划分Stage(阶段)的依据:对于宽依赖,必须等到上一阶段计算完成才能计算下一阶段</p> <h3 id="_8-dag是什么-localhost-4040-✔"><a href="#_8-dag是什么-localhost-4040-✔" class="header-anchor">#</a> 8.DAG是什么（localhost:4040）✔</h3> <p>DAG(DirectedAcyclicGraph有向无环图)指的是数据转换执行的过程，有方向，无闭环(<strong>其实就是RDD执行的流程</strong>)；</p> <p><strong>原始的RDD通过一系列的转换操作就形成了DAG有向无环图</strong>，任务执行时，可以按照DAG的描述，执行真正的计算 (数据被操作的一个过程)。</p> <h3 id="_9-dag中为什么要划分stage"><a href="#_9-dag中为什么要划分stage" class="header-anchor">#</a> 9.DAG中为什么要划分Stage</h3> <p><strong>并行计算</strong></p> <p>一个复杂的业务逻辑如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依 赖上一个阶段的数据。那么我们按照shuffle进行划分(也就是按照宽依赖进行划分)，就可以将一个DAG划分成多个Stage/ 1. 2. 1. 阶段，在同一个Stage中，会有多个算子操作，可以形成一个pipeline流水线，流水线内的多个平行的分区可以并行执行。</p> <h3 id="_10-如何划分dag的stage"><a href="#_10-如何划分dag的stage" class="header-anchor">#</a> 10.如何划分DAG的stage</h3> <p><strong>对于窄依赖</strong>，partition的转换处理在stage中完成计算，不划分(将窄依赖尽量放在在同一个stage中，可以实现流水线计算)。</p> <p><strong>对于宽依赖</strong>，由于有shuffle的存在，只能在父RDD处理完成后，才能开始接下来的计算，也就是说需要要划分stage。</p> <h3 id="_11-spark中的oom的问题"><a href="#_11-spark中的oom的问题" class="header-anchor">#</a> 11.Spark中的OOM的问题</h3> <ul><li>Map类型算子
<ul><li>增加堆内内存</li> <li>减少每个task处理的数量</li></ul></li> <li>Shuffle后
<ul><li>修改实际代码</li></ul></li> <li>Driver
<ul><li>将大对象转换成Executor端加载，增加driver端内存</li></ul></li></ul> <h3 id="_12-spark中数据的位置被谁管理的"><a href="#_12-spark中数据的位置被谁管理的" class="header-anchor">#</a> 12.Spark中数据的位置被谁管理的</h3> <ul><li><code>blockManger</code></li></ul> <p>每个数据分片都对于具体物理位置，<strong>数据的位置是被blockManager管理</strong>，无论数据是在磁盘，内存还是tacyan，都是由blockManager管理。</p> <h3 id="_13-spark程序执行-有时候默认为什么会产生很多task-怎么修改默认task执行个数"><a href="#_13-spark程序执行-有时候默认为什么会产生很多task-怎么修改默认task执行个数" class="header-anchor">#</a> 13.Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数</h3> <ul><li><code>SPARK_HOME/conf/spark-default.conf</code> <ul><li><code>spark.sql.shuffle.partitions=50</code></li> <li><code>spark.default.paralleism=10</code></li></ul></li></ul> <h3 id="_14-spark与mapreduce的shuffle的区别"><a href="#_14-spark与mapreduce的shuffle的区别" class="header-anchor">#</a> 14.Spark与MapReduce的Shuffle的区别</h3> <ul><li>MapReduce默认排序，Spark不排序，可以使用sortByKey</li> <li>MapReduce只有Map和Reduce两个阶段，比较局限，Spark更灵活</li> <li>MapReduce落盘，spark不落盘</li></ul> <h3 id="_17-通常来说-spark与mapreduce相比-spark运行效率更高。请说明效率更高来源于spark内置的哪些机制"><a href="#_17-通常来说-spark与mapreduce相比-spark运行效率更高。请说明效率更高来源于spark内置的哪些机制" class="header-anchor">#</a> 17.通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制</h3> <ul><li>DAG</li> <li>Linage</li> <li>基于内存</li></ul> <h3 id="_18-hadoop和spark的相同点和不同点"><a href="#_18-hadoop和spark的相同点和不同点" class="header-anchor">#</a> 18.Hadoop和Spark的相同点和不同点</h3> <ul><li>Hadoop
<ul><li>基于MapReduce架构的计算框架</li> <li>只有map和reduce两种操作，表达能力比较欠缺</li> <li>适合高延迟环境下批处理计算</li></ul></li> <li>Spark
<ul><li>基于内存的分布式计算框架</li> <li>有丰富的算子，比较灵活</li> <li>适合低延迟环境下的计算</li></ul></li></ul> <h3 id="_19-hadoop和spark使用场景"><a href="#_19-hadoop和spark使用场景" class="header-anchor">#</a> 19.Hadoop和Spark使用场景</h3> <ul><li>离线计算
<ul><li>Hadoop：单次数据量特别大</li> <li>Spark：单次数据量不是特别大</li></ul></li></ul> <h3 id="_20-spark如何保证宕机迅速恢复"><a href="#_20-spark如何保证宕机迅速恢复" class="header-anchor">#</a> 20.Spark如何保证宕机迅速恢复</h3> <ul><li>增加master备节点</li> <li>编写shell脚本，时刻检测存活状态，定期<strong>重启</strong></li></ul> <h3 id="_21-rdd持久化-缓存-原理✔"><a href="#_21-rdd持久化-缓存-原理✔" class="header-anchor">#</a> 21.RDD持久化（缓存）原理✔</h3> <ul><li>将数据持久化到内存上</li> <li><code>cache（） -&gt; persist()</code></li> <li><code>un cache（）</code></li></ul> <h3 id="_22-checkpoint检查点机制"><a href="#_22-checkpoint检查点机制" class="header-anchor">#</a> 22.Checkpoint检查点机制</h3> <ul><li>把应用数据存储到HDFS等可靠文件系统中（三个步骤）
<ul><li><code>setCheckPointDIR()</code></li> <li><code>CheckPoint()</code></li> <li>写入</li></ul></li></ul> <h3 id="缓存和检查点区别✔"><a href="#缓存和检查点区别✔" class="header-anchor">#</a> 缓存和检查点区别✔</h3> <p>1）<strong>Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖</strong>。</p> <p>2）Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。</p> <p>3）建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</p> <h3 id="_23-checkpoint和持久化机制的区别"><a href="#_23-checkpoint和持久化机制的区别" class="header-anchor">#</a> 23.Checkpoint和持久化机制的区别</h3> <ul><li>RDD</li> <li>安全性</li> <li>建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</li></ul> <h3 id="_32-rdd有哪些缺陷"><a href="#_32-rdd有哪些缺陷" class="header-anchor">#</a> 32.RDD有哪些缺陷？</h3> <ul><li>不支持细粒度的写和更新操作</li> <li>不支持迭代计算</li></ul> <h3 id="_34-spark为什么比mapreduce快"><a href="#_34-spark为什么比mapreduce快" class="header-anchor">#</a> 34.Spark为什么比mapreduce快</h3> <ul><li>内存</li> <li>DAG</li> <li>Lineage（血缘）</li></ul> <h3 id="_35-数据本地性是在哪个环节确定的"><a href="#_35-数据本地性是在哪个环节确定的" class="header-anchor">#</a> 35.数据本地性是在哪个环节确定的</h3> <ul><li>DAG划分stage时</li></ul> <h3 id="_36-rdd的弹性表现在哪几点"><a href="#_36-rdd的弹性表现在哪几点" class="header-anchor">#</a> 36.RDD的弹性表现在哪几点</h3> <ul><li>内存磁盘，task、stage失败重试，checkpoint、persis缓存。。</li></ul> <h3 id="_37-spark的数据本地性有哪几种"><a href="#_37-spark的数据本地性有哪几种" class="header-anchor">#</a> 37. Spark的数据本地性有哪几种</h3> <ul><li>读取本地节点的数据</li> <li>读取本地硬盘的数据</li> <li>读取非本地节点的数据（一般不用）</li></ul> <h3 id="_38-spark为什么要持久化-一般什么场景下要进行persist操作"><a href="#_38-spark为什么要持久化-一般什么场景下要进行persist操作" class="header-anchor">#</a> 38.Spark为什么要持久化，一般什么场景下要进行persist操作</h3> <ul><li>Spark默认存放在内存中，分布式系统中出现错误，如果没有rdd依赖关系，需要从头计算</li> <li>计算非常耗时</li> <li>计算链条非常长，重新恢复要算很多步骤</li> <li>checkpoint前需要进行持久化，下次就不需要重新计算</li> <li>shuffle之前要进行持久化persist，将数据持久化到磁盘</li></ul> <h3 id="_39-描述yarn执行一个任务的过程-☆☆☆☆☆-【hadoop"><a href="#_39-描述yarn执行一个任务的过程-☆☆☆☆☆-【hadoop" class="header-anchor">#</a> 39.描述Yarn执行一个任务的过程？（☆☆☆☆☆）【hadoop</h3> <h3 id="_40-spark-on-yarn-模式有哪些优点-☆☆☆☆☆-√"><a href="#_40-spark-on-yarn-模式有哪些优点-☆☆☆☆☆-√" class="header-anchor">#</a> 40.Spark on Yarn 模式有哪些优点？（☆☆☆☆☆）√</h3> <ul><li>与其他计算框架共享集群资源</li> <li>Yarn的资源分配更加细致</li> <li>Application部署简化</li> <li>资源弹性管理</li></ul> <h3 id="_41-谈谈你对container的理解-☆☆☆☆☆"><a href="#_41-谈谈你对container的理解-☆☆☆☆☆" class="header-anchor">#</a> 41.谈谈你对container的理解？（☆☆☆☆☆）</h3> <ul><li>资源分配和调度的基本单位</li> <li>由谁申请</li> <li>由谁执行</li></ul> <h3 id="_42-spark使用parquet文件存储格式能带来哪些好处-☆☆☆☆☆"><a href="#_42-spark使用parquet文件存储格式能带来哪些好处-☆☆☆☆☆" class="header-anchor">#</a> 42.Spark使用parquet文件存储格式能带来哪些好处？（☆☆☆☆☆）</h3> <ul><li>速度更快</li> <li>压缩计算非常稳定出色</li> <li>极大的减少磁盘I/O</li> <li>可以极大的优化spark的调度和执行</li></ul> <h3 id="_43-介绍parition和block有什么关联关系-☆☆☆☆☆"><a href="#_43-介绍parition和block有什么关联关系-☆☆☆☆☆" class="header-anchor">#</a> 43.介绍parition和block有什么关联关系？（☆☆☆☆☆）</h3> <ul><li>Spark，partition是rdd的最小单位，大小不固定，位于计算空间</li> <li>HDFS，block是分布式存储的最小单位，大小固定，位于存储空间</li></ul> <h3 id="_44-不需要排序的hash-shuffle是否一定比需要排序的sort-shuffle速度快"><a href="#_44-不需要排序的hash-shuffle是否一定比需要排序的sort-shuffle速度快" class="header-anchor">#</a> 44.不需要排序的hash shuffle是否一定比需要排序的sort shuffle速度快？</h3> <ul><li>数据规模大时，sort shuffle 速度快于hash shuffle</li></ul> <h3 id="_45-sort-based-shuffle的缺陷-☆☆☆☆☆"><a href="#_45-sort-based-shuffle的缺陷-☆☆☆☆☆" class="header-anchor">#</a> 45.Sort-based shuffle的缺陷? （☆☆☆☆☆）</h3> <ul><li>数量过大，会产生很多小文件</li></ul> <h3 id="_46-spark-storage-memoryfraction参数的含义-实际生产中如何调优"><a href="#_46-spark-storage-memoryfraction参数的含义-实际生产中如何调优" class="header-anchor">#</a> 46.spark.storage.memoryFraction参数的含义,实际生产中如何调优？</h3> <p>（☆☆☆☆☆）</p> <ul><li>RDD持久化在Executor内存中的占比（默认0.6）</li></ul> <h3 id="_47-介绍一下你对unified-memory-management内存管理模型的理解-☆☆☆☆☆"><a href="#_47-介绍一下你对unified-memory-management内存管理模型的理解-☆☆☆☆☆" class="header-anchor">#</a> 47.介绍一下你对Unified Memory Management内存管理模型的理解？（☆☆☆☆☆）</h3> <h3 id="_48-spark有哪两种算子✔"><a href="#_48-spark有哪两种算子✔" class="header-anchor">#</a> 48.Spark有哪两种算子✔</h3> <ul><li><strong>action算子</strong></li></ul> <p>（1）reduce
（2）collect
（3）count
（4）first
（5）take
（6）takeOrdered
（7）aggregate
（8）fold
（9）countByKey
（10）save
（11）foreach</p> <ul><li><strong>Transformation</strong></li></ul> <p>1）单Value
（1）map
（2）mapPartitions
（3）mapPartitionsWithIndex
（4）flatMap
（5）glom
（6）groupBy
（7）filter
（8）sample
（9）distinct
（10）coalesce
（11）repartition
（12）sortBy
（13）pipe
2）双vlaue
（1）intersection
（2）union
（3）subtract
（4）zip
3）Key-Value
（1）partitionBy
（2）reduceByKey
（3）groupByKey
（4）aggregateByKey
（5）foldByKey
（6）combineByKey
（7）sortByKey
（8）mapValues
（9）join
（10）cogroup</p> <h3 id="_49-spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子"><a href="#_49-spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子" class="header-anchor">#</a> 49.Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？</h3> <ul><li>reduceByKey，join，distinct，reparation</li> <li><strong>避免有shuffle的算子·：reduceBykey，groupByKey，ByKey:</strong></li></ul> <h3 id="_50-rdd创建有哪几种方式"><a href="#_50-rdd创建有哪几种方式" class="header-anchor">#</a> 50.RDD创建有哪几种方式？</h3> <ul><li>基于本地、集合、hdfs、数据库、非关系型数据库。。。</li></ul> <h3 id="_51-spark并行度怎么设置比较合适"><a href="#_51-spark并行度怎么设置比较合适" class="header-anchor">#</a> 51.Spark并行度怎么设置比较合适？</h3> <ul><li><strong>partition</strong>=（<strong>2~4</strong>）x <strong>core</strong></li></ul> <h3 id="_52-spark如何处理不能被序列化的对象"><a href="#_52-spark如何处理不能被序列化的对象" class="header-anchor">#</a> 52.Spark如何处理不能被序列化的对象？</h3> <ul><li>封装成object对象</li></ul> <h3 id="_53-collect功能是什么-其底层是怎么实现的"><a href="#_53-collect功能是什么-其底层是怎么实现的" class="header-anchor">#</a> 53.collect功能是什么，其底层是怎么实现的？</h3> <ul><li><strong>把各个节点的数据收集过来	Array-&gt;Tuple(k,v)</strong></li></ul> <h3 id="_54-为什么spark-application在没有获得足够的资源-job就开始执行了-可能会导致什么问题发生"><a href="#_54-为什么spark-application在没有获得足够的资源-job就开始执行了-可能会导致什么问题发生" class="header-anchor">#</a> 54.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么问题发生？</h3> <ul><li><strong>导致job结束也没有分配足够的资源</strong></li> <li><strong>task的调度线程和Executor资源申请是异步的</strong></li></ul> <h3 id="_55-map与flatmap的区别"><a href="#_55-map与flatmap的区别" class="header-anchor">#</a> 55.map与flatMap的区别？</h3> <ul><li>map
<ul><li>将一行数据转换成一个数组对象</li></ul></li> <li>flatMap
<ul><li>将所有对象合并成一个对象</li></ul></li></ul> <h3 id="_56-spark-on-mesos中-什么是的粗粒度分配-什么是细粒度分配-各自的优点和缺点是什么"><a href="#_56-spark-on-mesos中-什么是的粗粒度分配-什么是细粒度分配-各自的优点和缺点是什么" class="header-anchor">#</a> 56.Spark on Mesos中，什么是的粗粒度分配，什么是细粒度分配，各自的优点和缺点是什么？</h3> <ul><li>粗粒度
<ul><li>启动时就分配好资源</li> <li>优点：复用率高</li> <li>缺点：资源浪费</li></ul></li> <li>细粒度
<ul><li>用资源的时候分配</li> <li>优点：用完就回收，不浪费</li> <li>缺点：启动一次分配一次，比较麻烦</li></ul></li></ul> <h3 id="_57-driver的功能是什么"><a href="#_57-driver的功能是什么" class="header-anchor">#</a> 57.driver的功能是什么？</h3> <h3 id="_58-spark技术栈有哪些组件-每个组件都有什么功能-适合什么应用场景"><a href="#_58-spark技术栈有哪些组件-每个组件都有什么功能-适合什么应用场景" class="header-anchor">#</a> 58.Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3> <h3 id="_59-spark中worker的主要工作是什么"><a href="#_59-spark中worker的主要工作是什么" class="header-anchor">#</a> 59.Spark中Worker的主要工作是什么？</h3> <h3 id="_60-cache和pesist的区别✔"><a href="#_60-cache和pesist的区别✔" class="header-anchor">#</a> 60.cache和pesist的区别✔</h3> <ul><li>cache调用pesist</li></ul> <p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运 行时间</p> <p>1） cache只有一个默认的缓存级别MEMORY_ONLY ，cache调用了persist，而persist可以根据情况设置其它的缓存级别；</p> <p>2）executor执行的时候，默认60%做cache，40%做task操作，persist是最根本的函数，最底层的函数。</p> <h3 id="_61-cache后面能不能接其他算子-它是不是action操作"><a href="#_61-cache后面能不能接其他算子-它是不是action操作" class="header-anchor">#</a> 61.cache后面能不能接其他算子,它是不是action操作</h3> <ul><li>可以，但是起不到缓存的效果</li> <li>不是</li></ul> <h3 id="_62-reducebykey是不是action"><a href="#_62-reducebykey是不是action" class="header-anchor">#</a> 62.reduceByKey是不是action？</h3> <ul><li>不是，reduce是</li></ul> <h3 id="_63-rdd通过linage-记录数据更新-的方式为何很高效"><a href="#_63-rdd通过linage-记录数据更新-的方式为何很高效" class="header-anchor">#</a> 63.RDD通过Linage（记录数据更新）的方式为何很高效？</h3> <ul><li>lazy记录了数据的来源</li> <li>记录原数据</li> <li>简化复杂度</li></ul> <h3 id="_64-为什么要进行序列化"><a href="#_64-为什么要进行序列化" class="header-anchor">#</a> 64.为什么要进行序列化？</h3> <ul><li>减少存储空间，存储体积，高效存储和传输</li> <li>缺点：非常耗费CPU，需要进行反序列化</li></ul> <h3 id="kryo序列化"><a href="#kryo序列化" class="header-anchor">#</a> Kryo序列化</h3> <p>Kryo序列化比Java序列化更快更紧凑，但Spark默认的序列化是Java序列化并不是Spark序列化，因为Spark并不支持所有序列化类型，而且每次使用都必须进行注册。注册只针对于RDD。在DataFrames和DataSet当中自动实现了Kryo序列化。</p> <h3 id="_65-yarn中的container是由谁负责销毁的-在hadoop-mapreduce中container可以复用么"><a href="#_65-yarn中的container是由谁负责销毁的-在hadoop-mapreduce中container可以复用么" class="header-anchor">#</a> 65. Yarn中的container是由谁负责销毁的，在Hadoop Mapreduce中container可以复用么？</h3> <ul><li><code>ApplicationMaster</code></li> <li>mr中不可复用，spark on yarn 中可以复用</li></ul> <h3 id="_66-提交任务时-如何指定spark-application的运行模式"><a href="#_66-提交任务时-如何指定spark-application的运行模式" class="header-anchor">#</a> 66.提交任务时，如何指定Spark Application的运行模式？</h3> <ul><li><code>./spark-submit --class xx.xx.xx --master yarn --deploy-mode **cluster** xx.jar</code></li> <li><code>./spark-submit --class xx.xx.xx --master yarn --deploy-mode **client** xx.jar</code></li></ul> <h3 id="_67-不启动spark集群master和work服务-可不可以运行spark程序"><a href="#_67-不启动spark集群master和work服务-可不可以运行spark程序" class="header-anchor">#</a> 67.不启动Spark集群Master和work服务，可不可以运行Spark程序？</h3> <ul><li>可以，只有资源管理器由第三方管理就可以，如Yarn。resourceManager相当于master，NodeManager相当于Worker，做计算的是Executor，所以只要安装JVM就可以。</li></ul> <h3 id="_68-spark-on-yarn-cluster-模式下-applicationmaster和driver是在同一个进程么"><a href="#_68-spark-on-yarn-cluster-模式下-applicationmaster和driver是在同一个进程么" class="header-anchor">#</a> 68.spark on yarn Cluster 模式下，ApplicationMaster和driver是在同一个进程么？</h3> <ul><li>是</li></ul> <h3 id="_69-运行在yarn中application有几种类型的container"><a href="#_69-运行在yarn中application有几种类型的container" class="header-anchor">#</a> 69.运行在yarn中Application有几种类型的container？</h3> <ul><li>运行ApplicationMaster的Container</li> <li>运行各类任务的Container</li></ul> <h3 id="_70-executor启动时-资源通过哪几个参数指定"><a href="#_70-executor启动时-资源通过哪几个参数指定" class="header-anchor">#</a> 70.Executor启动时，资源通过哪几个参数指定？</h3> <ul><li>Executor
<ul><li><code>num-executors</code>是executor的数量，默认2</li> <li><code>executor-memory</code>是每个executor使用的内存，默认1G</li> <li><code>executor-cores</code>是每个executor分配的CPU，默认1，官方建议2</li></ul></li> <li>Driver
<ul><li><code>driver-cores</code> driver使用内核数，默认为1</li> <li><code>diver-memory</code> driver内存大小，默认512M</li></ul></li></ul> <h3 id="_71-导致executor产生full-gc-的原因-可能导致什么问题"><a href="#_71-导致executor产生full-gc-的原因-可能导致什么问题" class="header-anchor">#</a> 71.导致Executor产生FULL gc 的原因，可能导致什么问题？</h3> <ul><li>导致<code>executor</code>僵死问题</li></ul> <h3 id="_72-spark累加器有哪些特点"><a href="#_72-spark累加器有哪些特点" class="header-anchor">#</a> 72.Spark累加器有哪些特点？</h3> <ul><li>全局唯一，自增不减</li> <li>executor端修改，driver端读取</li> <li>executor级别共享</li></ul> <h3 id="_73-spark-hashparitioner的弊端是什么"><a href="#_73-spark-hashparitioner的弊端是什么" class="header-anchor">#</a> 73.spark hashParitioner的弊端是什么？</h3> <ul><li>数据倾斜</li></ul> <h3 id="_74-rangepartitioner分区的原理"><a href="#_74-rangepartitioner分区的原理" class="header-anchor">#</a> 74.RangePartitioner分区的原理？</h3> <ul><li>尽量保证每个分区中数据量的均匀，分区与分区之间有序，分区内无序</li></ul> <h3 id="_75-rangepartioner分区器特点"><a href="#_75-rangepartioner分区器特点" class="header-anchor">#</a> 75.rangePartioner分区器特点？</h3> <h3 id="_76-如何理解standalone模式下-spark资源分配是粗粒度的"><a href="#_76-如何理解standalone模式下-spark资源分配是粗粒度的" class="header-anchor">#</a> 76.如何理解Standalone模式下，Spark资源分配是粗粒度的？</h3> <ul><li>在提交时就分配好资源</li></ul> <h3 id="_77-union操作是产生宽依赖还是窄依赖"><a href="#_77-union操作是产生宽依赖还是窄依赖" class="header-anchor">#</a> 77.union操作是产生宽依赖还是窄依赖？</h3> <ul><li>窄依赖</li></ul> <h3 id="_78-窄依赖父rdd的partition和子rdd的parition是不是都是一对一的关系"><a href="#_78-窄依赖父rdd的partition和子rdd的parition是不是都是一对一的关系" class="header-anchor">#</a> 78.窄依赖父RDD的partition和子RDD的parition是不是都是一对一的关系？</h3> <ul><li>不一定</li></ul> <h3 id="_79-hadoop中-mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子"><a href="#_79-hadoop中-mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子" class="header-anchor">#</a> 79.Hadoop中，Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？</h3> <ul><li>map</li> <li>reduceByKey</li></ul> <h3 id="_80-什么是shuffle-以及为什么需要shuffle"><a href="#_80-什么是shuffle-以及为什么需要shuffle" class="header-anchor">#</a> 80.什么是shuffle，以及为什么需要shuffle？</h3> <ul><li>将具有共同特征是数据聚合到同一个计算节点上进行计算</li></ul> <h3 id="_81-spark中的hashshufle的有哪些不足"><a href="#_81-spark中的hashshufle的有哪些不足" class="header-anchor">#</a> 81.Spark中的HashShufle的有哪些不足？</h3> <ul><li>产生大量小文件在磁盘上</li> <li>内存不够用</li> <li>数据倾斜</li></ul> <h3 id="_82-conslidate是如何优化hash-shuffle时在map端产生的小文件"><a href="#_82-conslidate是如何优化hash-shuffle时在map端产生的小文件" class="header-anchor">#</a> 82.conslidate是如何优化Hash shuffle时在map端产生的小文件？</h3> <ul><li><code>conslidate</code>是根据CPU的个数决定每个task shuffle map 端产生多少个文件</li> <li><code>Hash shuffle</code>是根据reduce数量决定每个task shuffle map 端产生多少个文件</li></ul> <h3 id="_83-spark-default-parallelism这个参数有什么意义-实际生产中如何设置"><a href="#_83-spark-default-parallelism这个参数有什么意义-实际生产中如何设置" class="header-anchor">#</a> 83.spark.default.parallelism这个参数有什么意义，实际生产中如何设置？</h3> <ul><li>stage是默认task个数</li></ul> <h3 id="_84-spark-shuffle-memoryfraction参数的含义-以及优化经验"><a href="#_84-spark-shuffle-memoryfraction参数的含义-以及优化经验" class="header-anchor">#</a> 84.spark.shuffle.memoryFraction参数的含义，以及优化经验？</h3> <ul><li>shuffle从上一个task拉取数据过来，要在Executor进行聚 合操作，聚合操作时使用Executor内存的比例由该参数决定</li></ul> <h3 id="_85-spark中standalone模式特点-有哪些优点和缺点"><a href="#_85-spark中standalone模式特点-有哪些优点和缺点" class="header-anchor">#</a> 85.Spark中standalone模式特点，有哪些优点和缺点？</h3> <ul><li>特点
<ul><li>主从架构</li> <li>FIFO调度</li> <li>无需依赖其他资源管理系统</li></ul></li> <li>优点
<ul><li>无需依赖其他资源管理系统</li> <li>部署简单</li></ul></li> <li>缺点
<ul><li>一个应用程序会占用所有可用节点的资源</li> <li>容易出现单点故障，需要配置HA</li></ul></li></ul> <h3 id="_86-常见的数压缩方式-你们生产集群采用了什么压缩方式-提升了多少效率"><a href="#_86-常见的数压缩方式-你们生产集群采用了什么压缩方式-提升了多少效率" class="header-anchor">#</a> 86.常见的数压缩方式，你们生产集群采用了什么压缩方式，提升了多少效率？</h3> <ul><li>GZ	压缩率最高，压缩和解压速度最慢</li> <li>LZO</li> <li>snappy</li></ul> <h3 id="_87-使用scala代码实现wordcount"><a href="#_87-使用scala代码实现wordcount" class="header-anchor">#</a> 87.使用scala代码实现WordCount？</h3> <ul><li><code>new SparkContext( new Sparkconf() )</code></li> <li><code>.textFlie(&quot;XX.txt&quot;)</code></li> <li><code>.flatMap(.split(&quot; &quot;))</code></li> <li><code>.map( (,1) )</code></li> <li><code>.reduceByKey(_+_)</code></li> <li><code>.collect()</code></li> <li><code>.foreach(println)</code></li></ul> <h3 id="_89-rdd的数据结构是怎么样的✔"><a href="#_89-rdd的数据结构是怎么样的✔" class="header-anchor">#</a> 89.RDD的数据结构是怎么样的✔</h3> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/29617954/1677899267708-a79d4ddd-95b4-4f7f-85ef-b53e62fec7d0.png#averageHue=%23f5f2f1&amp;clientId=uaa6c32e4-fb4e-4&amp;from=paste&amp;height=784&amp;id=uf1c59f94&amp;originHeight=980&amp;originWidth=1823&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=362133&amp;status=done&amp;style=none&amp;taskId=ua529e298-06cc-486d-b09b-5545c70b3cf&amp;title=&amp;width=1458.4" alt="image.png"></p> <h3 id="_90-rdd算子里操作一个外部map-比如往里面put数据-然后算子外再遍历map-会有什么问题吗"><a href="#_90-rdd算子里操作一个外部map-比如往里面put数据-然后算子外再遍历map-会有什么问题吗" class="header-anchor">#</a> 90.RDD算子里操作一个外部map，比如往里面put数据，然后算子外再遍历map，会有什么问题吗？</h3> <h3 id="_91-说说你对hadoop生态的认识。"><a href="#_91-说说你对hadoop生态的认识。" class="header-anchor">#</a> 91.说说你对Hadoop生态的认识。</h3> <ul><li>分布式文件系统</li> <li>分布式计算引擎</li> <li>周边工具</li></ul> <h3 id="_92-hbase-region多大会分区-spark读取hbase数据是如何划分partition的"><a href="#_92-hbase-region多大会分区-spark读取hbase数据是如何划分partition的" class="header-anchor">#</a> 92.hbase region多大会分区，spark读取hbase数据是如何划分partition的？</h3> <ul><li>参数默认1G。HBASE有多少个region，spark就会划分几个partition</li></ul> <h3 id="_93-spark数据倾斜"><a href="#_93-spark数据倾斜" class="header-anchor">#</a> 93.Spark数据倾斜</h3> <ul><li>随机取样
<ul><li>相同key的值进行统计个数，将倾斜的过滤掉</li></ul></li> <li>两两聚合
<ul><li>给key加随机数，直接聚合，取消随机数，再次聚合</li></ul></li> <li>大表和小表
<ul><li>reduce join-&gt;map join，将小表广播出去，然后大表操作</li></ul></li> <li>大表和大表
<ul><li>一个大表扩容，一个大表key随机加前缀，将key均匀分布到不同的分区中，并行处理</li></ul></li></ul> <h3 id="_94-spark中taskschedule的作用是啥"><a href="#_94-spark中taskschedule的作用是啥" class="header-anchor">#</a> 94.Spark中TaskSchedule的作用是啥</h3> <ul><li>taskSchedule将task发送给executor</li></ul> <h3 id="_95-spark的内存分布"><a href="#_95-spark的内存分布" class="header-anchor">#</a> 95.Spark的内存分布</h3> <ul><li>堆内内存
<ul><li>会发生OOM</li></ul></li> <li>堆外内存
<ul><li>物理内存</li></ul></li></ul> <h3 id="_96-spark处理小文件"><a href="#_96-spark处理小文件" class="header-anchor">#</a> 96.Spark处理小文件</h3> <ul><li>减少分区
<ul><li><code>coalesce</code></li> <li><code>reparation</code></li></ul></li></ul> <h3 id="_97-spark如何实现报警功能"><a href="#_97-spark如何实现报警功能" class="header-anchor">#</a> 97.spark如何实现报警功能</h3> <ul><li>写代码获取yarn类的spark状态</li> <li>写代码获取executor、CPU、内存等信息，来判断存活状态</li></ul> <h3 id="_98-spark的shuffle✔"><a href="#_98-spark的shuffle✔" class="header-anchor">#</a> 98.Spark的shuffle✔</h3> <ul><li>hashShuffle</li> <li>sortShuffle</li></ul> <h3 id="哪些算子会引起shuffle✔"><a href="#哪些算子会引起shuffle✔" class="header-anchor">#</a> 哪些算子会引起shuffle✔</h3> <p>一般 xxbykey 会shuffle，因为涉及到数据重分区</p> <p>repartition：底层调用的就是coalesce，一定会shuffle，一般用于增大分区</p> <p>coalesce：不一定会shuffle，一般用于缩小分区</p> <p>合并小文件：启动一个Spark任务，读取，使用coalesce</p> <h3 id="_99-spark中oom"><a href="#_99-spark中oom" class="header-anchor">#</a> 99.Spark中OOM</h3> <h3 id="_100-reparation和coalesce关系与区别✔"><a href="#_100-reparation和coalesce关系与区别✔" class="header-anchor">#</a> 100.Reparation和Coalesce关系与区别✔</h3> <ul><li>调整分区（涉及shuffle）</li> <li>减少分区</li></ul> <p>关系:</p> <ul><li>两者都是用来改变RDD的partition数量的，repartition底层调用的就是coalesce方法:coalesce(numPartitions, shuffle = true)</li></ul> <p>区别:</p> <ul><li>repartition一定会发生shuffle，coalesce根据传入的参数来判断是否发生shuffle</li> <li>一般情况下增大rdd的partition数量使用repartition，减少partition数量时使用coalesce</li></ul> <h3 id="_101-yarn-client的提交流程"><a href="#_101-yarn-client的提交流程" class="header-anchor">#</a> 101.Yarn-client的提交流程</h3> <ul><li>Driver在任务提交的本地运行</li></ul> <h3 id="_102-yarn-cluster的提交流程"><a href="#_102-yarn-cluster的提交流程" class="header-anchor">#</a> 102.Yarn-cluster的提交流程</h3> <ul><li>Driver在ApplicationMaster上运行</li></ul> <h3 id="_103-spark调优原则✔"><a href="#_103-spark调优原则✔" class="header-anchor">#</a> 103.Spark调优原则✔</h3> <ul><li>避免使用重复RDD</li> <li>尽量使用同一个RDD</li> <li>避免使用有shuffle的算子</li> <li>对多次使用的RDD进行持久化</li> <li>mapSide预聚合</li> <li>使用高性能是算子</li> <li>广播大变量</li></ul> <h3 id="_105-map和mappartitions区别✔"><a href="#_105-map和mappartitions区别✔" class="header-anchor">#</a> 105.map和mapPartitions区别✔</h3> <p>1）map：每次处理一条数据
2）mapPartitions：每次处理一个分区数据</p> <h3 id="_106-reducebykey、foldbykey、aggregatebykey、combinebykey区别✔"><a href="#_106-reducebykey、foldbykey、aggregatebykey、combinebykey区别✔" class="header-anchor">#</a> 106. reduceByKey、foldByKey、aggregateByKey、combineByKey区别✔</h3> <p>ReduceByKey     没有初始值         分区内和分区间逻辑相同</p> <p>foldByKey      有初始值           分区内和分区间逻辑相同</p> <p>aggregateByKey   有初始值           分区内和分区间逻辑可以不同</p> <p>combineByKey   初始值可以变化结构  分区内和分区间逻辑不同</p> <h3 id="_107-spark任务的划分✔"><a href="#_107-spark任务的划分✔" class="header-anchor">#</a> 107.Spark任务的划分✔</h3> <p>（1）Application：初始化一个SparkContext即生成一个Application；</p> <p>（2）Job：一个Action算子就会生成一个Job；</p> <p>（3）Stage：Stage等于宽依赖的个数加1；</p> <p>（4）Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。</p> <h3 id="_108-cache缓存级别✔"><a href="#_108-cache缓存级别✔" class="header-anchor">#</a> 108.cache缓存级别✔</h3> <p>DataFrame的cache默认采用 MEMORY_AND_DISK
RDD 的cache默认方式采用MEMORY_ONLY</p> <h3 id="_109-spark分区"><a href="#_109-spark分区" class="header-anchor">#</a> 109.Spark分区</h3> <p>1）默认采用Hash分区
缺点：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。
2）Ranger分区
要求RDD中的KEY类型必须可以排序。
3）自定义分区
根据需求，自定义分区。</p> <h2 id="sparksql"><a href="#sparksql" class="header-anchor">#</a> SparkSQL</h2> <h3 id="_15-sparksql执行的流程"><a href="#_15-sparksql执行的流程" class="header-anchor">#</a> 15.SparkSql执行的流程</h3> <h3 id="_16-saprksql是如何将数据写到hive表的"><a href="#_16-saprksql是如何将数据写到hive表的" class="header-anchor">#</a> 16.SaprkSql是如何将数据写到hive表的</h3> <ul><li><p>SparkRDDApi  -&gt; HDFS	<strong>映射</strong></p></li> <li><p>SparkSql		-&gt; DataFrame	-&gt;	缓存表	<strong>直接插入</strong></p></li> <li></li></ul> <h3 id="_88-spark-sql为什么比hive快呢"><a href="#_88-spark-sql为什么比hive快呢" class="header-anchor">#</a> 88.Spark sql为什么比hive快呢？</h3> <h3 id="_110-sparksql中rdd、dataframe、dataset三者的转换-笔试重点"><a href="#_110-sparksql中rdd、dataframe、dataset三者的转换-笔试重点" class="header-anchor">#</a> 110.SparkSQL中RDD、DataFrame、DataSet三者的转换（笔试重点）</h3> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/29617954/1677901144741-fbaca06c-0175-4390-aa70-12e31b950bd3.png#averageHue=%23fefdfd&amp;clientId=u8869e584-7142-4&amp;from=paste&amp;height=294&amp;id=uef542093&amp;originHeight=368&amp;originWidth=798&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=28132&amp;status=done&amp;style=none&amp;taskId=u1bcf9a38-76ee-42ef-8717-b12af26f863&amp;title=&amp;width=638.4" alt="image.png"></p> <h2 id="sparkstreaming"><a href="#sparkstreaming" class="header-anchor">#</a> SparkStreaming</h2> <h3 id="_24-sparkstreaming以及基本工作原理"><a href="#_24-sparkstreaming以及基本工作原理" class="header-anchor">#</a> 24.SparkStreaming以及基本工作原理？</h3> <h3 id="_25-dstream以及基本工作原理"><a href="#_25-dstream以及基本工作原理" class="header-anchor">#</a> 25.DStream以及基本工作原理？</h3> <h3 id="_26-sparkstreaming整合kafka的两种模式"><a href="#_26-sparkstreaming整合kafka的两种模式" class="header-anchor">#</a> 26.SparkStreaming整合Kafka的两种模式？</h3> <ul><li>Receiver
<ul><li>主动去SparkExecutor的内存中处理数据</li></ul></li> <li>Direct
<ul><li>定期查询kafka中的offset，拉取新数据，进行处理</li></ul></li></ul> <h3 id="_27-spark主备切换原理知道吗"><a href="#_27-spark主备切换原理知道吗" class="header-anchor">#</a> 27.Spark主备切换原理知道吗？</h3> <ul><li>基于文件系统，手动切换</li> <li>基于ZooKeeper，自动切换</li></ul> <h3 id="_28-spark解决了hadoop的哪些问题"><a href="#_28-spark解决了hadoop的哪些问题" class="header-anchor">#</a> 28.Spark解决了Hadoop的哪些问题？</h3> <ul><li>硬编码</li> <li>表达局限</li> <li>一个Job一个MapReduce阶段</li> <li>存储内存-&gt;磁盘</li> <li>窄依赖并行处理</li> <li>实时计算</li></ul> <h3 id="_29-sparkmasterha主从切换过程会不会影响到集群已有的作业的运行-为什么"><a href="#_29-sparkmasterha主从切换过程会不会影响到集群已有的作业的运行-为什么" class="header-anchor">#</a> 29.SparkMasterHA主从切换过程会不会影响到集群已有的作业的运行，为什么？</h3> <ul><li>不会，已经分配好资源了</li></ul> <h3 id="_30-sparkmaster使用zookeeper进行ha-有哪些源数据保存到zookeeper里面"><a href="#_30-sparkmaster使用zookeeper进行ha-有哪些源数据保存到zookeeper里面" class="header-anchor">#</a> 30.SparkMaster使用Zookeeper进行HA,有哪些源数据保存到Zookeeper里面？</h3> <ul><li>master的元数据Metastore</li></ul> <h3 id="_31-如何实现sparkstreaming读取flume中的数据"><a href="#_31-如何实现sparkstreaming读取flume中的数据" class="header-anchor">#</a> 31.如何实现SparkStreaming读取Flume中的数据？</h3> <ul><li>推模式</li> <li>拉模式</li></ul></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/learn-ceiling/handbook/Interview/12-Hive.html" class="prev">
        Hive
      </a></span> <span class="next"><a href="/learn-ceiling/handbook/Interview/16-数据采集工具.html">
        数据采集工具
      </a>
      →
    </span></p></div> </main></div> <div class="footer" data-v-34d4abe6><div id="footer-info" data-v-34d4abe6><div class="footer-left" data-v-34d4abe6>
      © 2023-2024
      <span data-v-34d4abe6><a href="https://github.com/25122lxy/" target="_blank">lxy25122</a></span></div> <div class="footer-right" data-v-34d4abe6>Theme:<a href="https://github.com/JoeyBling/vuepress-theme-yilia-plus" target="_blank">vuepress-theme-yilia-plus</a> by Litten</div></div> <!----></div></main><div class="global-ui"><div id="live2d-widget" class="live2d-widget-container" style="position:fixed;right:65px;bottom:0px;width:135px;height:300px;z-index:99999;opacity:0.8;pointer-events:none;"><canvas id="live2d_canvas" width="135" height="300" class="live2d_canvas" style="position:absolute;left:0px;top:0px;width:135px;height:300px;"></canvas></div><canvas id="vuepress-canvas-cursor"></canvas></div></div>
    <script src="/learn-ceiling/assets/js/app.235538a9.js" defer></script><script src="/learn-ceiling/assets/js/2.c7d2b1f7.js" defer></script><script src="/learn-ceiling/assets/js/21.f4e6e20a.js" defer></script>
  </body>
</html>
